# Configuration for MRPC RoBERTa LoRA SGLD Experiment
# Based on experiment outline in sgld_lora_experiment.md

experiment:
  name: "mrpc_roberta_lora_sgld"
  description: "SGLD for Bayesian LoRA on MRPC dataset"
  
model:
  backbone: "roberta-base"
  max_sequence_length: 64  # Moderately increased for better performance
  lora:
    inject_into: ["query", "key", "value", "attention.output.dense"]  # Only attention modules
    rank: 4  # Increased for better expressiveness
    alpha: 8  # Increased proportionally with rank
    dropout: 0.1

data:
  dataset: "mrpc"
  batch_size: 4  # Ultra-reduced for GPU memory constraints
  gradient_accumulation_steps: 8  # Maintain effective batch size of 32
  max_epochs: 20

training:
  map_lora:
    optimizer: "adamw"
    learning_rate: 5e-5
    weight_decay: 0.01
    beta1: 0.9
    beta2: 0.999
    scheduler: "linear"
    warmup_ratio: 0.1
  
  sgld_lora:
    learning_rate: 5e-4  # Reduced for better stability
    temperature: 1.0
    prior_std: 0.01  # Very strong prior to balance likelihood
    step_size_schedule:
      initial: 5e-4  # Reduced initial step size for stability
      decay_rate: 0.5  # Slower decay for more stable sampling
      decay_steps: 2000  # Longer decay schedule
    gradient_clip_norm: 0.5  # Tighter gradient clipping
    chains: 3  # Restored for R-hat computation
    burn_in_steps: 5000  # Much longer burn-in for convergence
    sampling_steps: 10000  # Many more samples for better ESS
    thinning: 50  # Much higher thinning for independent samples
    samples_to_retain: 200

evaluation:
  metrics: ["accuracy", "nll", "ece"]
  mcmc_diagnostics:
    probe_set_size: 512
    summaries: ["log_posterior", "l2_norm"]
    r_hat_threshold: 1.05
    ess_threshold: 200

output:
  save_dir: "runs/mrpc_roberta_lora_sgld"
  save_samples: true
  save_traces: true
