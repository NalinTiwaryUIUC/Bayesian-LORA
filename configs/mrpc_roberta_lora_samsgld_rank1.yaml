# Configuration for MRPC RoBERTa LoRA SAM-SGLD Rank-1 Experiment
# Based on experiment outline in sgld_lora_experiment.md

experiment:
  name: "mrpc_roberta_lora_samsgld_rank1"
  description: "SAM-SGLD Rank-1 for Bayesian LoRA on MRPC dataset"
  
model:
  backbone: "roberta-base"
  max_sequence_length: 256
  lora:
    inject_into: ["query", "key", "value", "attention.output.dense"]  # Only attention modules
    rank: 8  # Increased for better expressiveness
    alpha: 16  # Increased proportionally with rank
    dropout: 0.1

data:
  dataset: "mrpc"
  batch_size: 32  # Optimized for best convergence and GPU utilization
  gradient_accumulation_steps: 1  # No longer needed with larger batch size
  max_epochs: 20  # Full training for proper MAP model

training:
  map_lora:
    optimizer: "adamw"  # JUSTIFIED: AdamW achieves 82% vs SGD's 68% MAP accuracy
    learning_rate: 2e-4  # Standard Adam learning rate
    weight_decay: 0.01
    beta1: 0.9  # Adam momentum decay
    beta2: 0.999  # Adam squared gradient decay
    eps: 1e-8  # Adam epsilon for numerical stability
    scheduler: "none"  # Disable scheduler to avoid LR decay
    warmup_ratio: 0.0  # Disable warmup
  
  samsgld_rank1_lora:
    learning_rate: 0.002  # Matches initial step size for consistency
    temperature: 2.0  # Increased to balance step/noise ratio (was 1.5, too low)
    prior_std: 17.0  # Will be properly balanced using correct ratio calculation
    step_size_schedule:
      initial: 0.002  # JUSTIFIED: Drift/noise ratio 0.115 is too low, need smaller step size
      decay_rate: 0.3  # Conservative decay for better convergence
      decay_steps: 4000  # Match burn-in steps for proper decay schedule
    gradient_clip_norm: 5.0  # Match SGLD clipping (was 1.0)
    noise_scale: 0.0001  # JUSTIFIED: Reduce noise to improve drift/noise ratio from 0.115
    # SAM-SGLD specific parameters
    rho: 0.05  # Slightly reduced to prevent excessive SAM perturbation
    lambd: 0.00000001  # SAM regularization (1e-8)
    sigma_dir: 8.0  # JUSTIFIED: Increase from 4.0 to achieve ~10% rank-1 noise contribution (was ~5%)
    chains: 4  # Multi-chain for proper R-hat computation and multimodal assessment
    burn_in_steps: 8000  # Full experiment: Restored from 1500 for proper burn-in
    sampling_steps: 3000  # Full experiment: Restored from 500 for adequate sampling
    thinning: 50  # Full experiment: Restored from 10 for proper thinning
    samples_to_retain: 60  # 1 chain * 60 samples = 60 total samples

evaluation:
  metrics: ["accuracy", "nll", "ece"]
  mcmc_diagnostics:
    probe_set_size: 512
    summaries: ["log_posterior", "l2_norm"]
    r_hat_threshold: 1.05  # Not used for multimodal assessment
    ess_threshold: 50  # Used for within-mode mixing (threshold/4 = 12.5)
    track_min_ess: true  # Track minimum ESS across all chains and metrics

output:
  save_dir: "runs/mrpc_roberta_lora_samsgld_rank1"
  save_samples: true
  save_traces: true
